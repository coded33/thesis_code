{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brief Overview Over the Results for NELA-GT-2018 (Horne et al.) and EuroParl (Nanni et al.) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Before reading this notebook, make sure to read the README.md file first in order to gain some basic understanding of this repository's structure. This notebook will also only include a brief overview over the results, with the details and further information left to the thesis, out of which some of the text and graphics in this overview notebook are taken from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the NELA-GT-2018 (Horne) notebooks we created synthetic articles with each article containing a fixed number of words. We looked at the cumulative distribution of the article word counts in the dataset in order to determine the article size. These numbers have shown themselves to be quite consistent throughout all the notebook, we thus take an example for the left articles from the horne_l_r (NELA-GT-2018 (Horne) Left-Right) notebook:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"width:50em\" src='./overview_img/word_count_1.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we have quite extreme outliers in our dataset. We thus removed all articles with less than 50 and more than 5000 words, as those may be faulty data in the dataset. In any case, they would not be what we would recognize as an article. As a result we ended up with this distribution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"width:50em\" src='./overview_img/word_count_2.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the elbow method, we decided for an article length of 1000 words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the EuroParl dataset we did not create new synthetic articles, however, we also applied the two different ways of preprocessing described below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For certain methods, we want to provide text which has not been altered by typical NLP processes such as, for example, stemming. The basic preprocessing is applied to all input texts – it is the minimum amount of alteration the methods receive. \n",
    "\n",
    "Since we want to start with the most natural text, we will remove extra whitespace, special symbols, numbers and layout syntax for all methods. This means we remove everything except the symbols ' ? ! . : , and small and capital letters in order to create our base data for all methods. The reason for that is that such basic punctuation is actually part of the word embeddings used by Nanni et al. in their SemScale paper. We use these word embeddings in all notebooks. They find their application in SemScale itself, but also in the feed-forward networks and linear regression, as we run them separately on the word embeddings and on token counts. Additionally, even though not using the word embeddings from Nanni et al, BERT can also interpret punctuation. It must be noted that while we limit our experiments to texts of the English language, some texts include letters from foreign languages, like the letter é, or other symbols which some of our methods would not be able to handle well. We thus try to transform the text to something that most closely resembles a natural text of the English language, using only the English alphabet and only having reasonable whitespaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For methods which are based on word semantics we apply further text preprocessing. We take as input the basic preprocessing output. A reduced space of words means we actually count the frequencies for the base words, and not count separate frequencies for variations of words, which would impede our methods in interpreting the data correctly. We thus use the following further preprocessing steps:\n",
    "\n",
    "- **Lowercasing**: Set all upper case letters to lower case. This helps us reduce the count of words.\n",
    "\n",
    "\n",
    "- **Removal of punctuation and numbers**:  Punctuation and numbers are not of benefit for our word frequency methods, as they give little semantic information when their count per document appears in a document-feature matrix or a token-count vector.\n",
    "\n",
    "\n",
    "- **Removal of stopwords**: Stopwords such as \"the\" or \"is\" and others are not words containing a direct sentiment on their own and are thus removed.\n",
    "\n",
    "\n",
    "- **Stemming**: Words in the texts are pruned to their base form, e.g. \"runner\" and \"running\" may be pruned to \"run\". This is already an example of what problems stemming may cause (e.g. turning verbs into nouns), however, it greatly reduces the number of words inside a text corpus and is thus beneficial for our word frequency methods.\n",
    "\n",
    "\n",
    "- **Removal of extra whitespace**: Through previous preprocessing and through faults in the dataset, extra whitespace in the text may exist which is removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "We start off with showing how the methods performed on their own and how they performed on each notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"width:50em\" src='./overview_img/overview_mse.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"width:50em\" src='./overview_img/overview_pearson.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A discussion of the results is in the thesis, however what can be noted here is that neither machine learning methods, nor political science methods clearly outperform the other. Interestingly, with the increase of variety in the target labels in the NELA-GT notebooks, the MSE improves, also the combined EuroParl notebook performs better than the one with the individual legislations. This is an indication that more data is always better. However, we also found some problems concerning the suitability of the metrics. These and further things are discussed in more detail in the thesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at our output plots, the red lines mark 0 and 1 on the Y-axis. We put these lines there in order to make it more visible for the reader when a method creates scores which are outside the range of the target labels (which goes from 0 to 1). The blue dots are the scores for documents, with the X-Axis indicating the \"actual leaning\" and the Y-Axis the predicted leaning. The blue line indicates a fitted regression line, while the light-blue shade surrounding it is the confidence interval of the regression. Ideally the outputs lie on the diagonal from (0,0) to (1,1), as that would indicate a perfect correlation between target and predicted scores.\n",
    "\n",
    "We follow with showing how the outputs of the methods ideally look like, and an example of how they actually look like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"width:50em\" src='./overview_img/ideal_vs_example.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now show the first fold of every method in every notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"width:50em\" src='./overview_img/outputs_1.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"width:50em\" src='./overview_img/outputs_2.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random vs. Real Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now show how some methods' output clearly differs when being given real data vs. random data. However, some methods do not differ much in their output when compared to being given random data, thus hinting at their unsuitedness for the task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"width:50em\" src='./overview_img/random_comparison.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token Counts vs. Word Embeddings\n",
    "\n",
    "We were able to find out that token counts outperformed the word embeddings also used in the SemScale paper by Nanni et al., which is probably because the embeddings were not tailored (enough) for the task. More on that in the thesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"width:50em\" src='./overview_img/token_vs_embs.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SemScale Paper Replication\n",
    "We now show a comparison of the paper by Nanni et al. the creators of SemScale and our results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"width:50em\" src='./overview_img/repli_nanni.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure above taken from \n",
    "*Nanni, Federico, et al. \"Political text scaling meets computational semantics.\" arXiv preprint arXiv:1904.06217 (2019)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"width:50em\" src='./overview_img/repli_mine.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"width:50em\" src='./overview_img/sem_table.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By comparing our results to the ones of Nanni et al. (cited above) we get reminded again of how important the reproducibility of methods is (which could not be established here for reasons named in the thesis), but we were also able to observe that preprocessing impacts the results differently depending on the method, as it can both improve (like with Wordfish), but also worsen the results (like with SemScale), depending on the data and the methods. Future research could dig deeper into this. More on this topic in the thesis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
